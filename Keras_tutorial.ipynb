{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Course in Neural Networks with Keras\n",
    "This is a brief introduction to using Keras, which will also teach you a bit about neural networks along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras, glob\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Neural Network = Model\n",
    "In broad terms machine learning is teaching a machine the relation between some input, $x$ and some output $y$, so \n",
    "\n",
    "$y = f(x, a) $\n",
    "\n",
    "where $a$ is a set of parameters that define the function $f$.\n",
    "\n",
    "A neural network can therefore be thought of as a model $f$ that has a set of parameters, $a$, that you try to optimize to best predict the relation between, a set of training images and their labels. \n",
    "\n",
    "Neural networks can also be used for regression problems, and very simplified analogy would be a very complicated, high-order polynomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential models\n",
    "Most neural networks, or models, you can construct with Keras fall under the sequential model type. This is the 'typical' network seen in most illustrations, and consists of a sequence of layers that each take inputs from neurons in the preceding layer, perform some operation depending on the layer type, and then feed it forward into the next layer in the sequence.\n",
    "\n",
    "In 'classical' code, this would look something like:\n",
    "```python\n",
    "y = input(x)\n",
    "y = layer1_func(y)\n",
    "y = layer2_func(y)\n",
    "y = output(y)\n",
    "```\n",
    "\n",
    "Keras gives access to many types of layers, corresponding to different arithmetic functions and array manipulations. Keras allows you to mix and match these layers in your sequence, and will automatically sort out the connections between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising a sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "This sets up a simple image classifier, to classify images of cats and dogs.\n",
    "\n",
    "### Setting up data\n",
    "#### Training data\n",
    "To set up a training set, all you need to do it create a directory, where sub-directories correspond to each class. \n",
    "\n",
    "Example:\n",
    "\n",
    "/home/training_set/cats\n",
    "\n",
    "/home/training_set/dogs\n",
    "\n",
    "Keras then only needs to be pointed at /home/training_set and it will figure out the classes, in this case cats and dogs.\n",
    "\n",
    "#### Validation data\n",
    "The training data that the network never sees.\n",
    "\n",
    "#### Testing data\n",
    "The data neither you or the network have seen. Performance on this is what you put in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you have placed your training data\n",
    "training_dir = './training_set' # EDIT THIS\n",
    "validation_dir = './validation_set' # EDIT THIS\n",
    "testing_dir = './test_set' # EDIT THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "Data generators are functions used for ingesting data and feeding it into your network, with some degree of manupulations along the way. They are typically used to feed in small batches of the training data, so as to conserve memory in case the training set is very large.\n",
    "\n",
    "For image classification Keras has a built in class ImageDataGenerator that does this, and also preprocesses the images, and can perform data augmentation. \n",
    "\n",
    "#### Preprocessing\n",
    "Image data comes in a variety of different shapes, sizes and formats. Your network will only function or at least function well, on images that are preprocessed in a similar fashion to what it's been trained on. It's therefore useful to establish a format specific for your network. These include, rescaling the pixel values to be between 0 and 255 (png standard), binning or interpolating the image to a specific shape. \n",
    "\n",
    "#### Data Augmentation\n",
    "In general purpose classification it is sometimes useful to alter the training data in some way. This makes the network able to handle a greater variation in the unseen data. It can also be used to artificially increase the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2470 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image preprocessing\n",
    "image_dim = (64, 64) # Image dimensions, important for network arch.\n",
    "rescale = 1./255 # Rescale pixel values to minimum 0, maximum 255\n",
    "\n",
    "# Data Augmentation\n",
    "shear_range = 0.2 # Randomly shearing the image alonge either axis\n",
    "zoom_range = 0.2 # Randomly zoom in on parts of the image \n",
    "horizontal_flip = True # Randomly flip images horizontaly\n",
    "\n",
    "# Training-time parameters\n",
    "batchsize = 32 # no. of images generator yields each call\n",
    "shuffle = True # randomize the training set\n",
    "class_mode = 'categorical' # specific to network type, classifiers can be 'binary' or 'categorical'\n",
    "\n",
    "# Initialize the ImageDataGenerator class\n",
    "train_datagen = ImageDataGenerator(rescale = rescale, \n",
    "                                   shear_range = shear_range, \n",
    "                                   zoom_range = zoom_range,\n",
    "                                   horizontal_flip = horizontal_flip) \n",
    "\n",
    "# Setup the ImageDataGenerator instance to ingest images from training_dir\n",
    "training_set = train_datagen.flow_from_directory(training_dir,\n",
    "                                                 target_size = image_dim, \n",
    "                                                 batch_size = batchsize, \n",
    "                                                 shuffle = shuffle,\n",
    "                                                 class_mode = class_mode)\n",
    "\n",
    "\n",
    "# Initialize the ImageDataGenerator class\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Setup the ImageDataGenerator instance to ingest images from training_dir\n",
    "validation_set = validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size = image_dim,\n",
    "                                                        batch_size = batchsize,\n",
    "                                                        class_mode = class_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "conv_kernel = (3,3)\n",
    "model.add(Conv2D(32, conv_kernel, input_shape = (image_dim[0], image_dim[1], 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.6683 - acc: 0.6031 - val_loss: 0.7432 - val_acc: 0.5225\n",
      "Epoch 2/25\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.6084 - acc: 0.6576 - val_loss: 0.6089 - val_acc: 0.6813\n",
      "Epoch 3/25\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.5893 - acc: 0.6794 - val_loss: 0.6168 - val_acc: 0.6663\n",
      "Epoch 4/25\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.5703 - acc: 0.7072 - val_loss: 0.6548 - val_acc: 0.6300\n",
      "Epoch 5/25\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.5598 - acc: 0.7108 - val_loss: 0.7392 - val_acc: 0.5925\n",
      "Epoch 6/25\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.5425 - acc: 0.7256 - val_loss: 0.6515 - val_acc: 0.6488\n",
      "Epoch 7/25\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.5277 - acc: 0.7411 - val_loss: 0.6811 - val_acc: 0.6350\n",
      "Epoch 8/25\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.5204 - acc: 0.7324 - val_loss: 0.7161 - val_acc: 0.6375\n",
      "Epoch 9/25\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.5045 - acc: 0.7569 - val_loss: 0.6895 - val_acc: 0.6488\n",
      "Epoch 10/25\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.4992 - acc: 0.7580 - val_loss: 0.5761 - val_acc: 0.7125\n",
      "Epoch 11/25\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.4858 - acc: 0.7707 - val_loss: 0.8162 - val_acc: 0.6138\n",
      "Epoch 12/25\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.4749 - acc: 0.7674 - val_loss: 0.6803 - val_acc: 0.6750\n",
      "Epoch 13/25\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.5005 - acc: 0.7653 - val_loss: 0.6209 - val_acc: 0.6937\n",
      "Epoch 14/25\n",
      "77/77 [==============================] - 7s 88ms/step - loss: 0.4731 - acc: 0.7645 - val_loss: 0.5936 - val_acc: 0.7113\n",
      "Epoch 15/25\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.4563 - acc: 0.7932 - val_loss: 0.7215 - val_acc: 0.6637\n",
      "Epoch 16/25\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.4586 - acc: 0.7820 - val_loss: 0.6924 - val_acc: 0.6763\n",
      "Epoch 17/25\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.4504 - acc: 0.7901 - val_loss: 0.5869 - val_acc: 0.7087\n",
      "Epoch 18/25\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 0.4599 - acc: 0.7877 - val_loss: 0.6998 - val_acc: 0.6775\n",
      "Epoch 19/25\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 0.4634 - acc: 0.7675 - val_loss: 0.6574 - val_acc: 0.7013\n",
      "Epoch 20/25\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.4535 - acc: 0.7678 - val_loss: 0.6436 - val_acc: 0.7050\n",
      "Epoch 21/25\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 0.4555 - acc: 0.7835 - val_loss: 0.6841 - val_acc: 0.6813\n",
      "Epoch 22/25\n",
      "27/77 [=========>....................] - ETA: 2s - loss: 0.4341 - acc: 0.7870"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(training_set, \n",
    "                    steps_per_epoch = int(training_set.samples/batchsize), \n",
    "                    epochs = 25, \n",
    "                    validation_data = validation_set,\n",
    "                    validation_steps = int(validation_set.samples/batchsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots images with labels within jupyter notebook\n",
    "def plots(ims, figsize=(16,7), rows=3, interp=False, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(np.around(titles[i],2), fontsize=16)\n",
    "        plt.imshow(ims[i])\n",
    "\n",
    "\n",
    "# Initialize the ImageDataGenerator class\n",
    "testing_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Setup the ImageDataGenerator instance to ingest images from training_dir\n",
    "testing_set = testing_datagen.flow_from_directory(testing_dir,\n",
    "                                                  target_size = image_dim,\n",
    "                                                  batch_size = 1)\n",
    "N = 24    \n",
    "predicted_labels = np.zeros((N,2))\n",
    "test_images = []\n",
    "for i in range(N):\n",
    "    test_image, true_labels = next(testing_set)\n",
    "    test_images.append(test_image[0])\n",
    "    predicted_labels[i,:] = model.predict(test_image)#[0]    \n",
    "plots(test_images, titles=predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "N = testing_set.samples\n",
    "testing_set.reset()\n",
    "predicted_labels = np.zeros((N,2))\n",
    "true_labels = np.zeros((N,2))\n",
    "test_images = []\n",
    "for i in range(N):\n",
    "    test_image, true_labels[i,:] = next(testing_set)\n",
    "    test_images.append(test_image[0])\n",
    "    predicted_labels[i,:] = model.predict(test_image)[0]\n",
    "    \n",
    "for i,p in enumerate(predicted_labels):\n",
    "    predicted_labels[i][np.argmin(p)] = 0\n",
    "    predicted_labels[i][np.argmax(p)] = 1\n",
    "    \n",
    "cm = confusion_matrix(true_labels[:,0], predicted_labels[:,0])\n",
    "\n",
    "plot_confusion_matrix(cm, ['cat','dog'], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```history``` stores information for how metrics changed during training. These metrics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want training and validation to converge. \n",
    "\n",
    "- If validation accuracy > training accuracy: Underfitting\n",
    "- If validation accuracy < training accuracy: Overfitting\n",
    "\n",
    "Hyper-parameters need to be optimized to get the best fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    axl = fig.add_subplot(121)\n",
    "    axl.plot(history.history['val_loss'], label='validation')\n",
    "    axl.plot(history.history['loss'], label='training')\n",
    "    axl.set_ylabel('Loss')\n",
    "    axl.set_xlabel('Epoch')\n",
    "    \n",
    "    axa = axl = fig.add_subplot(122)\n",
    "    axa.plot(history.history['val_acc'], label='validation')\n",
    "    axa.plot(history.history['acc'], label='training')\n",
    "    axa.set_ylabel('Accuracy')\n",
    "    axa.set_xlabel('Epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters and Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are used to convert from an input signal of a node to an output signal. That output can then be used as an input to the next layer or as the output layer. They introduce non-linearity into our network which we need in order to make a flexible map from input to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for plots below\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "def plot_activation(x,A):    \n",
    "    ax = plt.gca()\n",
    "    ax.plot(x, A)\n",
    "    ax.grid(True)\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['top'].set_color('none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are activation functions in keras (or through a TensorFlow/Theano function). These can be added as a separate layer when you build your model, e.g.:\n",
    "```python\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "```\n",
    "\n",
    "or as an argument within each layer, e.g.:\n",
    "```python\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "```\n",
    "\n",
    "The options are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __LINEAR__:\n",
    "\n",
    "```python\n",
    "activation = 'linear'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = x\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = x\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __RELU__:\n",
    "```python\n",
    "activation = 'relu'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = \\begin{cases}\n",
    "x & \\text{if x>0,}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.where(x<0, 0, x)\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ELU__:\n",
    "\n",
    "```python\n",
    "activation = 'elu'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = \\begin{cases}\n",
    "x & \\text{if x>0,}\\\\\n",
    "\\alpha . (e^{x}-1) & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "A = np.where(x<0, alpha * (np.exp(x)-1), x)\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __TANH__:\n",
    "\n",
    "```python\n",
    "activation = 'tanh'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __SIGMOID__:\n",
    "\n",
    "```python\n",
    "activation = 'sigmoid'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1) / (1 + np.exp(-x))\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __SOFTMAX__:\n",
    "\n",
    "```python\n",
    "activation = 'softmax'\n",
    "```\n",
    "\n",
    "\\begin{equation*}\n",
    "A(x) = \\frac{e^{x}}{\\sum\\limits^k_{j=0} e^{x_j}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (np.exp(x)) / (np.sum(np.exp(x)))\n",
    "plot_activation(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "These are used to update the weight parameters in order to minimise the loss function. This is an argument which is required for compiling your model in keras, i.e.:\n",
    "```python\n",
    "from keras import optimizers\n",
    "optimizer = #you choose\n",
    "loss = #you choose\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done after you have built the layers of your model.\n",
    "\n",
    "Some of the Keras options are:\n",
    "- __Stochastic gradient descent__ (SGD)\n",
    "    ```\n",
    "    keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    ```\n",
    "- __RMSProp__\n",
    "    ```\n",
    "    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    ```\n",
    "- __Adam__ (popular choice, RMSProp + Momentum)\n",
    "    ```\n",
    "    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    ```\n",
    "- __Adagrad__ (learning rate per parameter)\n",
    "    ```\n",
    "    keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    ```\n",
    "- __Adadelta__ (adaptive learning rate per parameter)\n",
    "    ```\n",
    "    keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some optimizer arguments:\n",
    "- __learning rate__, ```lr=```, \n",
    "Parameter which controls how much the weights change with each update.\n",
    "- __momentum__, \n",
    "Parameter that accelerates SGD in the relevant direction and dampens oscillations.\n",
    "- __decay__,\n",
    "Makes the learning rate decay after each update (epoch).\n",
    "- __beta_1 & beta_2__, \n",
    "Parameters controling the adaptive decay rates per parameter. Keep close to 1.0.\n",
    "- __epsilon__,\n",
    "A small number to prevent any division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "The quantity to minimise to best fit the model. This is also required to compile your model (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the Keras options are:\n",
    "- __mean squared error__\n",
    "```\n",
    "keras.losses.mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "- __mean absolute error__\n",
    "```\n",
    "keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "- __binary cross entropy__\n",
    "```\n",
    "keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "```\n",
    "\n",
    "They all take two arguments: ```y_true```: true labels, and ```y_pred```: the predictions of the model. Any custom loss function must use the same arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other hyperparameters\n",
    "These are some example of arguments when fitting the model using ```model.fit()``` or ```model.fit_generator()```.\n",
    "- __Number of epochs__, A hyperparameter to balance between training for long enough but not taking ages!\n",
    "- __Callbacks__\n",
    "- __Batch size__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "A callback is a function which is applied during training. You can use callbacks to get a view on internal states and statistics of the model during training or to change some hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the Keras options are:\n",
    "- __BaseLogger__, accumulates epoch averages of metrics.\n",
    "```\n",
    "keras.callbacks.BaseLogger(stateful_metrics=None)\n",
    "```\n",
    "- __ModelCheckpoint__, saves the model after every epoch.\n",
    "```\n",
    "keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss')\n",
    "```\n",
    "- __EarlyStopping__, stops training when a monitored quantity has stopped improving.\n",
    "```\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0)\n",
    "```\n",
    "- __ReduceLROnPlateau__, reduces learning rate when a metric has stopped improving.\n",
    "```\n",
    "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "```\n",
    "- __LambdaCallback__, for customising simple callbacks on-the-fly.\n",
    "```\n",
    "keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, \n",
    "                                 on_batch_begin=None, on_batch_end=None, \n",
    "                                 on_train_begin=None, on_train_end=None)\n",
    "```\n",
    "\n",
    "Or you can make a custom callback (see Keras documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
